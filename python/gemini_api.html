<!DOCTYPE html>
<html lang="ja">
<head>
<link rel="canonical" href="https://okumuralab.org/~okumura/python/gemini_api.html">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Gemini APIを使う</title>
<link rel="stylesheet" href="style.css">
<link rel="license" href="https://creativecommons.org/licenses/by/4.0/">
<script src="../js/copypre.js"></script>
</head>
<body>

<nav id="breadcrumbs">
<a href="../">ホーム</a> &gt;
<a href="./">Python</a> &gt;
</nav>

<h1>Gemini APIを使う</h1>

<p><a href="https://ai.google.dev/tutorials/python_quickstart">Gemini API: Quickstart with Python</a>に従って、Pythonパッケージを</p>

<pre>
pip install google-generativeai
</pre>

<p>でインストールすれば、</p>

<pre class="cell">
import google.generativeai as genai
</pre>

<p>で使えるようになる。</p>

<p>Google AI Studioの<a href="https://makersuite.google.com/app/apikey">API keys</a>で「Create API key in new project」を選んでAPIキーを発行してもらう。キーは</p>

<pre>
export GOOGLE_API_KEY="..."
</pre>

<p>のようにして環境変数としておくと勝手に使ってもらえるが、うまくいかなければ</p>

<pre class="cell">
genai.configure(api_key="...")
</pre>

<p>で指定する。</p>

<pre class="cell">
list(genai.list_models())
</pre>

<p>と打ち込めば、使えるモデルが一覧できる。例：</p>

<pre>
[Model(name='models/chat-bison-001',
       base_model_id='',
       version='001',
       display_name='PaLM 2 Chat (Legacy)',
       description='A legacy text-only model optimized for chat conversations',
       input_token_limit=4096,
       output_token_limit=1024,
       supported_generation_methods=['generateMessage', 'countMessageTokens'],
       temperature=0.25,
       max_temperature=None,
       top_p=0.95,
       top_k=40),
 Model(name='models/text-bison-001',
       base_model_id='',
       version='001',
       display_name='PaLM 2 (Legacy)',
       description='A legacy model that understands text and generates text as an output',
       input_token_limit=8196,
       output_token_limit=1024,
       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],
       temperature=0.7,
       max_temperature=None,
       top_p=0.95,
       top_k=40),
 Model(name='models/embedding-gecko-001',
       base_model_id='',
       version='001',
       display_name='Embedding Gecko',
       description='Obtain a distributed representation of a text.',
       input_token_limit=1024,
       output_token_limit=1,
       supported_generation_methods=['embedText', 'countTextTokens'],
       temperature=None,
       max_temperature=None,
       top_p=None,
       top_k=None),
 Model(name='models/gemini-1.0-pro-latest',
       base_model_id='',
       version='001',
       display_name='Gemini 1.0 Pro Latest',
       description=('The best model for scaling across a wide range of tasks. This is the latest '
                    'model.'),
       input_token_limit=30720,
       output_token_limit=2048,
       supported_generation_methods=['generateContent', 'countTokens'],
       temperature=0.9,
       max_temperature=None,
       top_p=1.0,
       top_k=None),
 Model(name='models/gemini-1.0-pro',
       base_model_id='',
       version='001',
       display_name='Gemini 1.0 Pro',
       description='The best model for scaling across a wide range of tasks',
       input_token_limit=30720,
       output_token_limit=2048,
       supported_generation_methods=['generateContent', 'countTokens'],
       temperature=0.9,
       max_temperature=None,
       top_p=1.0,
       top_k=None),
 Model(name='models/gemini-pro',
       base_model_id='',
       version='001',
       display_name='Gemini 1.0 Pro',
       description='The best model for scaling across a wide range of tasks',
       input_token_limit=30720,
       output_token_limit=2048,
       supported_generation_methods=['generateContent', 'countTokens'],
       temperature=0.9,
       max_temperature=None,
       top_p=1.0,
       top_k=None),
 Model(name='models/gemini-1.0-pro-001',
       base_model_id='',
       version='001',
       display_name='Gemini 1.0 Pro 001 (Tuning)',
       description=('The best model for scaling across a wide range of tasks. This is a stable '
                    'model that supports tuning.'),
       input_token_limit=30720,
       output_token_limit=2048,
       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],
       temperature=0.9,
       max_temperature=None,
       top_p=1.0,
       top_k=None),
 Model(name='models/gemini-1.0-pro-vision-latest',
       base_model_id='',
       version='001',
       display_name='Gemini 1.0 Pro Vision',
       description='The best image understanding model to handle a broad range of applications',
       input_token_limit=12288,
       output_token_limit=4096,
       supported_generation_methods=['generateContent', 'countTokens'],
       temperature=0.4,
       max_temperature=None,
       top_p=1.0,
       top_k=32),
 Model(name='models/gemini-pro-vision',
       base_model_id='',
       version='001',
       display_name='Gemini 1.0 Pro Vision',
       description='The best image understanding model to handle a broad range of applications',
       input_token_limit=12288,
       output_token_limit=4096,
       supported_generation_methods=['generateContent', 'countTokens'],
       temperature=0.4,
       max_temperature=None,
       top_p=1.0,
       top_k=32),
 Model(name='models/gemini-1.5-pro-latest',
       base_model_id='',
       version='001',
       display_name='Gemini 1.5 Pro Latest',
       description='Mid-size multimodal model that supports up to 2 million tokens',
       input_token_limit=2097152,
       output_token_limit=8192,
       supported_generation_methods=['generateContent', 'countTokens'],
       temperature=1.0,
       max_temperature=2.0,
       top_p=0.95,
       top_k=64),
 Model(name='models/gemini-1.5-pro-001',
       base_model_id='',
       version='001',
       display_name='Gemini 1.5 Pro 001',
       description='Mid-size multimodal model that supports up to 2 million tokens',
       input_token_limit=2097152,
       output_token_limit=8192,
       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],
       temperature=1.0,
       max_temperature=2.0,
       top_p=0.95,
       top_k=64),
 Model(name='models/gemini-1.5-pro',
       base_model_id='',
       version='001',
       display_name='Gemini 1.5 Pro',
       description='Mid-size multimodal model that supports up to 2 million tokens',
       input_token_limit=2097152,
       output_token_limit=8192,
       supported_generation_methods=['generateContent', 'countTokens'],
       temperature=1.0,
       max_temperature=2.0,
       top_p=0.95,
       top_k=64),
 Model(name='models/gemini-1.5-pro-exp-0801',
       base_model_id='',
       version='exp-0801',
       display_name='Gemini 1.5 Pro Experimental 0801',
       description='Mid-size multimodal model that supports up to 2 million tokens',
       input_token_limit=2097152,
       output_token_limit=8192,
       supported_generation_methods=['generateContent', 'countTokens'],
       temperature=1.0,
       max_temperature=2.0,
       top_p=0.95,
       top_k=64),
 Model(name='models/gemini-1.5-pro-exp-0827',
       base_model_id='',
       version='exp-0827',
       display_name='Gemini 1.5 Pro Experimental 0827',
       description='Mid-size multimodal model that supports up to 2 million tokens',
       input_token_limit=2097152,
       output_token_limit=8192,
       supported_generation_methods=['generateContent', 'countTokens'],
       temperature=1.0,
       max_temperature=2.0,
       top_p=0.95,
       top_k=64),
 Model(name='models/gemini-1.5-flash-latest',
       base_model_id='',
       version='001',
       display_name='Gemini 1.5 Flash Latest',
       description='Fast and versatile multimodal model for scaling across diverse tasks',
       input_token_limit=1048576,
       output_token_limit=8192,
       supported_generation_methods=['generateContent', 'countTokens'],
       temperature=1.0,
       max_temperature=2.0,
       top_p=0.95,
       top_k=64),
 Model(name='models/gemini-1.5-flash-001',
       base_model_id='',
       version='001',
       display_name='Gemini 1.5 Flash 001',
       description='Fast and versatile multimodal model for scaling across diverse tasks',
       input_token_limit=1048576,
       output_token_limit=8192,
       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],
       temperature=1.0,
       max_temperature=2.0,
       top_p=0.95,
       top_k=64),
 Model(name='models/gemini-1.5-flash-001-tuning',
       base_model_id='',
       version='001',
       display_name='Gemini 1.5 Flash 001 Tuning',
       description='Fast and versatile multimodal model for scaling across diverse tasks',
       input_token_limit=16384,
       output_token_limit=8192,
       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],
       temperature=1.0,
       max_temperature=2.0,
       top_p=0.95,
       top_k=64),
 Model(name='models/gemini-1.5-flash',
       base_model_id='',
       version='001',
       display_name='Gemini 1.5 Flash',
       description='Fast and versatile multimodal model for scaling across diverse tasks',
       input_token_limit=1048576,
       output_token_limit=8192,
       supported_generation_methods=['generateContent', 'countTokens'],
       temperature=1.0,
       max_temperature=2.0,
       top_p=0.95,
       top_k=64),
 Model(name='models/gemini-1.5-flash-exp-0827',
       base_model_id='',
       version='exp-0827',
       display_name='Gemini 1.5 Flash Experimental 0827',
       description='Fast and versatile multimodal model for scaling across diverse tasks',
       input_token_limit=1048576,
       output_token_limit=8192,
       supported_generation_methods=['generateContent', 'countTokens'],
       temperature=1.0,
       max_temperature=2.0,
       top_p=0.95,
       top_k=64),
 Model(name='models/gemini-1.5-flash-8b-exp-0827',
       base_model_id='',
       version='001',
       display_name='Gemini 1.5 Flash 8B Experimental 0827',
       description='Fast and versatile multimodal model for scaling across diverse tasks',
       input_token_limit=1048576,
       output_token_limit=8192,
       supported_generation_methods=['generateContent', 'countTokens'],
       temperature=1.0,
       max_temperature=2.0,
       top_p=0.95,
       top_k=64),
 Model(name='models/embedding-001',
       base_model_id='',
       version='001',
       display_name='Embedding 001',
       description='Obtain a distributed representation of a text.',
       input_token_limit=2048,
       output_token_limit=1,
       supported_generation_methods=['embedContent'],
       temperature=None,
       max_temperature=None,
       top_p=None,
       top_k=None),
 Model(name='models/text-embedding-004',
       base_model_id='',
       version='004',
       display_name='Text Embedding 004',
       description='Obtain a distributed representation of a text.',
       input_token_limit=2048,
       output_token_limit=1,
       supported_generation_methods=['embedContent'],
       temperature=None,
       max_temperature=None,
       top_p=None,
       top_k=None),
 Model(name='models/aqa',
       base_model_id='',
       version='001',
       display_name='Model that performs Attributed Question Answering.',
       description=('Model trained to return answers to questions that are grounded in provided '
                    'sources, along with estimating answerable probability.'),
       input_token_limit=7168,
       output_token_limit=1024,
       supported_generation_methods=['generateAnswer'],
       temperature=0.2,
       max_temperature=None,
       top_p=1.0,
       top_k=40)]
</pre>

<p><code>models/</code> は省略できるようだ。2024-05-30から有料になる。<a href="https://ai.google.dev/pricing">値段</a>。</p>

<p><code>'gemini-1.5-pro-latest'</code> を使って何か聞いてみよう：</p>

<pre class="cell">
model = genai.GenerativeModel('gemini-1.5-pro-latest')

response = model.generate_content("What is the meaning of life?")
print(response.text)
</pre>

<p>もし検閲に引っかかるようなら <code>response.prompt_feedback</code> を表示してみれば詳細がわかる。</p>

<p>上の方法では一つしか質問できない。会話は、次のようにする。</p>

<pre class="cell">
model = genai.GenerativeModel('gemini-1.5-pro-latest')

chat = model.start_chat(history=[])
response = chat.send_message("What is your knowledge cutoff?")
print(response.text)
response = chat.send_message("What are reputable news sources?")
print(response.text)
...
</pre>

<p>会話全体は <code>chat.history</code> で参照できる。</p>

<pre class="cell">
for h in chat.history:
    print(f"[{h.role}]\n{h.parts[0].text}")
</pre>

<p>温度（ランダムさの度合い）などを指定することもできる：</p>

<pre class="cell">
config = genai.GenerationConfig(temperature=0)
model = genai.GenerativeModel('gemini-1.5-pro-latest',
                              generation_config=config)
</pre>

<hr>

<footer>
<p><a href="../" rel="author">奥村 晴彦</a></p>
<p>Last modified: <time>2024-08-28 17:37:19 JST</time></p>
</footer>
</body>
</html>
