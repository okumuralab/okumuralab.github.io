<!DOCTYPE html>
<html lang="ja">
<head>
<link rel="canonical" href="https://okumuralab.org/~okumura/python/scrape.html">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>スクレイピング</title>
<link rel="stylesheet" href="style.css">
<link rel="license" href="https://creativecommons.org/licenses/by/4.0/">
</head>
<body>

<nav id="breadcrumbs">
<a href="../">ホーム</a> &gt;
<a href="./">Python</a> &gt;
</nav>

<h1>スクレイピング</h1>

<p>Webスクレイピングの基本は，標準ライブラリ <a href="https://docs.python.org/3/library/urllib.request.html">urllib.request</a> または <a href="https://requests.readthedocs.io">Requests</a>（<code>pip install requests</code> でインストール）と，正規表現 <a href="https://docs.python.org/3/library/re.html">re</a> とである。より高レベルのライブラリとして <a href="https://www.crummy.com/software/BeautifulSoup/">Beautiful Soup</a>（<code>pip install beautifulsoup4</code> でインストール）がある。詳しくは<a href="https://vaaaaaanquish.hatenablog.com/entry/2017/06/25/202924">PythonでWebスクレイピングする時の知見をまとめておく</a>が参考になる。</p>

<p>例えば，このサイトにどれだけリンクがあるかを調べてみよう。まず urllib.request による方法：</p>

<pre>
<code>import urllib.request
import re

with urllib.request.urlopen('http://example.jp/') as f:
    s = f.read().decode('utf-8')
    a = re.findall('&lt;a href="(.*?)"', s)
</code></pre>

<p>requests による方法：</p>

<pre>
<code>import requests
import re

r = requests.get('http://example.jp/')
r.raise_for_status()  # エラーチェック
r.encoding = 'UTF-8'  # 'ISO-8859-1' に間違えられることがある
a = re.findall('&lt;a href="(.*?)"', r.text)
</code></pre>

<p>これで <code>a</code> には <code>['../', '../stat/', 'install.html', ...]</code> という配列が入る。</p>

<div class="note">
<ul>
  <li><code>r.encoding</code> のデフォルトは ISO-8859-1 であり HTML の <code>&lt;meta charset="UTF-8"&gt;</code> を見てくれない。自動で UTF-8 と判断してほしければサーバ側で <code>AddDefaultCharset UTF-8</code> と設定しなければならない。</li>
  <li>エラーチェックには <code>r.raise_for_status()</code> を呼び出す方法以外に <code>r.status_code</code> が200（正常）かどうか確かめる方法もある。</li>
</ul>
</div>

<p>Beautiful Soup による方法：</p>

<pre>
<code>import requests
from bs4 import BeautifulSoup

r = requests.get('http://example.jp/')
r.raise_for_status()  # エラーチェック
r.encoding = 'utf-8'
s = BeautifulSoup(r.text)
s                                       # HTML全体
s.select('a')                           # &lt;a ...&gt; タグ
[x.get('href') for x in s.select('a')]  # &lt;a ...&gt; タグの中のhref
</code></pre>

<p>一般に正規表現は想定外の HTML の書き方に対応できないので Beautiful Soup のような高レベルの方法が良いとされる。</p>

<p>バイナリファイルのダウンロードは例えば次のようにする：</p>

<pre>
<code>import requests
import pathlib

r = requests.get('http://example.jp/hoge.png')
pathlib.Path("hoge.png").write_bytes(r.content)
</code></pre>

<p>ほか，例えば <code>r.headers['Last-Modified']</code> で更新日時がわかるので，<code>os.utime()</code> でファイルのタイムスタンプを変えるといったことも可能であろう。</p>

<p>User Agent（ブラウザの種類）は <code>"python-requests/2.22.0"</code> のような感じで先方のログに残る。これが嫌なら適当に変えられる：</p>

<pre>
<code>r = requests.get('http://example.jp/',
    headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Safari/605.1.15'})
</code></pre>

<p>最後に，いらすとやさんから「科学」ジャンルの画像のサムネール（256x256）をスクレイプする簡単なコード例を挙げておく。カレントディレクトリ下に <code>irasutoya</code> というサブディレクトリがあるとする。</p>

<pre>
<code>import requests
import re
import pathlib
import time

path = pathlib.Path("irasutoya")
names = [re.sub("^.*/", "", str(n)) for n in path.iterdir()]

url = 'https://www.irasutoya.com/search/label/科学'
while True:
    print("Requesting", url)
    r = requests.get(url)
    if r.status_code != 200:
        print("Status:", r.status_code)
        break
    r.encoding = 'utf-8'
    a = re.findall('document.write\(bp_thumbnail_resize\("(.*?)"', r.text)
    for i in a:
        name = re.sub("^.*/", "", i)
        if name in names:
            continue
        names.append(name)
        png256 = re.sub('/s72-c/', '/s256-c/', i)
        print("Downloading", png256)
        time.sleep(1)  # 迷惑をかけないように必ず数秒待つ
        r1 = requests.get(png256)
        if r1.status_code != 200:
            print("Status:", r1.status_code)
            continue
        pathlib.Path("irasutoya/" + name).write_bytes(r1.content)
    m = re.search("&lt;a class='blog-pager-older-link' href='(.*?)'", r.text)
    if not m:
        break
    url = m.group(1)
</code></pre>

<p>さらについでに，いらすとやさんの<a href="https://www.irasutoya.com/2014/10/faceicons.html">いろいろな顔アイコン</a>（動物・モンスターを除く人間だけ）以下の256×256サムネールを全部取得してカレントディレクトリの <code>irasutoya2</code> サブディレクトリに入れる：</p>

<pre>
<code>path = pathlib.Path("irasutoya2")
names = [re.sub("^.*/", "", str(n)) for n in path.iterdir()]

urls = [
    'http://www.irasutoya.com/2013/10/blog-post_5077.html',
    'http://www.irasutoya.com/2013/10/blog-post_3974.html',
    'http://www.irasutoya.com/2013/10/blog-post_9098.html',
    'http://www.irasutoya.com/2013/10/blog-post_6907.html',
    'http://www.irasutoya.com/2013/10/blog-post_872.html',
    'http://www.irasutoya.com/2013/10/blog-post_8683.html',
    'http://www.irasutoya.com/2013/10/blog-post_2022.html',
    'http://www.irasutoya.com/2013/10/blog-post_1473.html',
    'http://www.irasutoya.com/2015/10/blog-post_59.html',
    'http://www.irasutoya.com/2015/10/blog-post_29.html',
    'http://www.irasutoya.com/2015/10/blog-post_405.html',
    'http://www.irasutoya.com/2015/10/blog-post_135.html' ]

for u in urls:
    print("Requesting", u)
    r = requests.get(u)
    if r.status_code != 200:
        print("Status:", r.status_code)
        break
    r.encoding = 'utf-8'
    a = re.findall('"([^"]*/s800/[^"]*)"', r.text)
    for i in a:
        name = re.sub("^.*/", "", i)
        if name in names:
            continue
        names.append(name)
        png256 = re.sub('/s800/', '/s256-c/', i)
        m = re.search('^//', png256)
        if m:
            png256 = 'http:' + png256
        print("Downloading", png256)
        time.sleep(1)
        r1 = requests.get(png256)
        if r1.status_code != 200:
            print("Status:", r1.status_code)
            continue
        pathlib.Path("irasutoya2/" + name).write_bytes(r1.content)
</code></pre>

<hr>

<p><a href="../" rel="author">奥村 晴彦</a></p>

<p>
<!-- hhmts start -->
Last modified: <time>2020-12-15 14:59:22</time>
<!-- hhmts end -->
</p>
</body>
</html>
