<!DOCTYPE html>
<html lang="ja">
<head>
<link rel="canonical" href="https://okumuralab.org/~okumura/python/encoding.html">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>文字コード</title>
<link rel="stylesheet" href="style.css">
<link rel="license" href="https://creativecommons.org/licenses/by/4.0/">
<script src="../js/copypre.js"></script>
</head>
<body>

<nav id="breadcrumbs">
<a href="../">ホーム</a> &gt;
<a href="./">Python</a> &gt;
</nav>

<h1>文字コード</h1>

<h2>文字化け</h2>

<p>文字が糸へんの難しい文字などに化けるのは、UTF-8のバイト列をShift JIS（またはそれをMicrosoftが拡張したCP932（コードページ932））として解釈されてしまったためである。</p>

<p>例えば「あいうえお」というUTF-8文字列は、メモリ中で次のように保存されている：</p>

<pre class="cell">
"あいうえお".encode("utf-8")
</pre>

<pre>
b'\xe3\x81\x82\xe3\x81\x84\xe3\x81\x86\xe3\x81\x88\xe3\x81\x8a'
</pre>

<p>つまり16進で <code>e3 81 82 e3 81 84 e3 81 86 e3 81 88 e3 81 8a</code> である。これをShift JISのバイト列と解釈すると、縺 (<code>e3 81</code>) ゅ (<code>82 e3</code>) ＞ (<code>81 84</code>) 縺 (<code>e3 81</code>) 解釈不能 (<code>86</code>) 縺 (<code>e3 81</code>) 医 (<code>88 e3</code>) ♀  (<code>81 8a</code>) となる。これらは</p>

<pre class="cell">
b'\xe3\x81'.decode("cp932")
</pre>

<pre>
'縺'
</pre>

<p>のようにして調べられる。まとめると、「あいうえお」は次のように化ける：</p>

<pre class="cell">
"あいうえお".encode("utf-8").decode("cp932", "replace")
</pre>

<pre>
'縺ゅ＞縺�縺医♀'
</pre>

<p>最後の <code>"replace"</code> は解釈できなかったバイトを <code>'�'</code> (U+FFFD REPLACEMENT CHARACTER) に置き換えるオプションで、これを <code>"ignore"</code> にすれば単に無視されて <code>'縺ゅ＞縺縺医♀'</code> になる。</p>

<p>このように化けたものを元に戻すには逆をすればよい：</p>

<pre class="cell">
'縺ゅ＞縺�縺医♀'.encode("cp932", "ignore").decode("utf-8", "replace")
</pre>

<pre>
'あい�えお'
</pre>

<p>完全には元に戻せない。</p>

<h2>文字コード変換</h2>

<p>文字コードは標準ライブラリの <a href="https://docs.python.org/3/library/codecs.html">codecs</a> モジュールで扱える。直接これを使わなくても、いろいろな関数が <code>encoding=...</code> オプションで文字コード指定を受け付ける。デフォルトはUTF-8である。例えばコマンドライン引数で与えたEUC-JPのファイルをUTF-8に変換して <code>utf8/</code> サブディレクトリに保存するには、次のようにすればよい。</p>

<pre>
import sys

def convert(source, target):
    with open(source, 'r', encoding='euc_jis_2004', errors='replace') as f:
        content = f.read()
    with open(target, 'w', encoding='utf-8') as f:
        f.write(content)

if __name__ == "__main__":
    for i in range(1, len(sys.argv)):
        convert(sys.argv[i], "utf8/" + sys.argv[i])
</pre>

<p>ここで <code>euc_jp</code> では①などの「機種依存文字」がうまく扱えないので <code>euc_jis_2004</code> を使う。それでも変換できない場合があるので、<code>errors='replace'</code> を付けてエラーで止まらないようにする。例えば ∑ (0xad 0xf4 → U+2211) は変換できなかった。iconv なら EUC-JP-MS とすれば変換できた。</p>

<pre>
'①'.encode('euc_jp')        # UnicodeEncodeError
'①'.encode('euc_jis_2004')  # b'\xad\xa1'
'∑'.encode('euc_jis_2004')  # UnicodeEncodeError
b'\xad\xa1'.decode('euc_jis_2004')  # '①'
b'\xad\xf4'.decode('euc_jis_2004')  # UnicodeDecodeError
</pre>

<p>同様に <code>shift_jis</code> は <code>cp932</code> にする。</p>

<pre>
'①'.encode('shift_jis')  # UnicodeEncodeError
'①'.encode('cp932')      # b'\x87@'
'∑'.encode('cp932')      # b'\x87\x94'
</pre>

<h2>ZIPアーカイブのファイル名文字化け対策</h2>

<p>ZIP アーカイブの操作は <a href="https://docs.python.org/3/library/zipfile.html">zipfile</a> パッケージでできる。ただ，日本語環境の Windows で作成したアーカイブは，ファイル名が CP932（Shift JIS の Microsoft 拡張）になっている。これを Mac や Linux で展開するには，UTF-8 に変換しなければならない。</p>

<p>Python 3 の zipfile モジュール（zipfile.py）では</p>

<pre>
            if zinfo.flag_bits &amp; 0x800:
                # UTF-8 filename
                fname_str = fname.decode("utf-8")
            else:
                fname_str = fname.decode("cp437")
</pre>

<p>のような処理がされている。コードページ 437（DOS Latin US）はオリジナルの IBM PC の文字セットである。したがって，これを UTF-8 に変換して展開するには，次のようにすればよさそうである（DoraTeX さんによる暗号化 ZIP 対応をマージした）：</p>

<pre>
#! /usr/bin/env python3

import sys
from zipfile import ZipFile
from getpass import getpass

with ZipFile(sys.argv[1]) as z:
    for zinfo in z.infolist():
        if zinfo.flag_bits &amp; 0x1:
            password = getpass('PASSWORD: ')
            z.setpassword(password.encode('utf-8'))
        if not zinfo.flag_bits &amp; 0x800:
            try:
                name = zinfo.filename.encode('cp437').decode('cp932')
            except:
                name = zinfo.filename.encode('cp437').decode('utf-8')
            zinfo.filename = name
        print("Extracting", zinfo.filename)
        z.extract(zinfo)
</pre>

<p>この19行ほどのファイルを <code>unzip.py</code> という名前で作成して実行可能にしておけば，<code>./unzip.py アーカイブ名.zip</code> で展開できる。</p>

<p>Mac の展開ツール（ZIP アーカイブをダブルクリックすると現れる）はファイル名の文字コードの自動判断ができる（うまくいかない場合があるという話も聞いた）。Windows 8 以降の展開ツールも大丈夫らしい。Windows 7 は <a href="https://support.microsoft.com/ja-jp/help/2704299/japanese-characters-in-file-names-are-displayed-as-garbled-text-after">KB2704299</a> で対応したらしい。Mac の /usr/bin/unzip にはファイル名変換機能はない（CentOS 7 の /usr/bin/unzip には -O cp932 オプションがある）。</p>

<p>[追記] DoraTeX さんが<a href="https://gist.github.com/doraTeX/0d79d8ff35710a27bc3e5176cc646b60">改良版</a>を作られた。</p>

<h2>NFC，NFD</h2>

<p>新しい macOS では APFS というファイルシステムが用いられるが，従来の Mac で用いられていた HFS+ というファイルシステムでは，ファイル名の Unicode 正規化がほぼ NFD という形式になっていた。例えばカレントディレクトリに「パンダ.txt」というファイルを作り，<a href="https://docs.python.org/3/library/pathlib.html">pathlib</a> でファイル名を読み出す：</p>

<pre>
!touch パンダ.txt  # カレントディレクトリに「パンダ.txt」を作る

import pathlib

path = pathlib.Path(".")
s = str(list(path.glob("*ン*.txt"))[0])
</pre>

<p>ところが</p>

<pre>
In [ ]: s
Out[ ]: 'パンダ.txt'

In [ ]: list(s)
Out[ ]: ['ハ', '゚', 'ン', 'タ', '゙', '.', 't', 'x', 't']

In [ ]: [hex(ord(c)) for c in s]
Out[ ]:
['0x30cf',
 '0x309a',
 '0x30f3',
 '0x30bf',
 '0x3099',
 '0x2e',
 '0x74',
 '0x78',
 '0x74']
</pre>

<p>このように濁点・半濁点が分離される。これが NFD である。このように直前の文字に結合して半濁点，濁点を付ける文字はそれぞれ COMBINING KATAKANA-HIRAGANA SEMI-VOICED SOUND MARK（U+309A），COMBINING KATAKANA-HIRAGANA VOICED SOUND MARK（U+3099）である。</p>

<p>通常の NFC に直すには <a href="https://docs.python.org/3/library/unicodedata.html">unicodedata</a> モジュールを使う：</p>

<pre>
import unicodedata

t = unicodedata.normalize('NFC', s)
</pre>

<p>すると，今度は</p>

<pre>
In [ ]: t
Out[ ]: 'パンダ.txt'

In [ ]: list(t)
Out[ ]: ['パ', 'ン', 'ダ', '.', 't', 'x', 't']
</pre>

<p>見かけは同じだが中身が違う。</p>

<p>別の例（Pokémon の é）：</p>

<pre>
In [ ]: list('é')
Out[ ]: ['é']

In [ ]: unicodedata.normalize('NFD', 'é')
Out[ ]: 'é'

In [ ]: list(unicodedata.normalize('NFD', 'é'))
Out[ ]: ['e', '́']
</pre>

<p>たとえAPFSでも，過去のHFS+からコピーしたファイルはNFDになっていることがあるので，要注意。</p>

<p>カレントディレクトリ以下にあるすべてのファイルについて，ファイル名をNFCに直す。<code>XR8L0bKA6Ofb</code> は他と重ならないファイル名なら何でもいい。なぜか <code>os.rename(s, t)</code> ではうまくいかなかった。</p>

<pre>
for x in pathlib.Path(".").glob("**/*"):
    s = str(x)
    t = unicodedata.normalize("NFC", s)
    if s != t:
        print("Renaming", s)
        os.rename(s, "XR8L0bKA6Ofb")
        os.rename("XR8L0bKA6Ofb", t)
</pre>

<p>Homebrewのrsyncは <code>--iconv=UTF8-MAC,UTF-8</code> というオプションでNFD→NFCにしてくれる。ただ，<code>-E</code> がmacOSのrsyncと違って "copy extended attributes, resource forks" の意味を持たないのが残念（<code>man -M /usr/share/man rsync</code> と <code>man -M /usr/local/share/man rsync</code> を比較）。</p>

<h2>「埼玉」と「埼⽟」</h2>

<p>総務省の<a href="https://www.soumu.go.jp/kojinbango_card/">マイナンバー制度とマイナンバーカード</a>にある「マイナンバーカード交付状況」PDFで「埼玉」（玉: U+7389）と「埼⽟」（⽟: U+2F5F）が混在しているという<a href="https://twitter.com/hal_sk/status/1281853581218336768">話</a>を聞いた。後者はCJK部首/康熙部首である。こういうのは <code>unicodedata.normalize('NFKC', '「埼玉」と「埼⽟」')</code> で統一できる（<code>'NFKD'</code> だと濁点などが分離する）。こういったものが混在する原因はAdobe DistillerまたはChromeのPDF出力である<a href="http://twitter.com/MurakamiShinyu/status/1282590800094756864">とのこと</a>。</p>

<p>NFKC化で全部の部首が正規化されるわけではないようだ。次のようにすればよいと<a href="https://twitter.com/imabari_ehime/status/1282605117468897280">教えていただいた</a>：</p>

<pre>
str.maketrans("⺟⺠⻁⻄⻑⻘⻝⻤⻨⻩⻫⻭⻯⻲戶", "母民虎西長青食鬼麦黄斉歯竜亀戸")
</pre>

<p>詳しくは<a href="https://imabari.hateblo.jp/entry/2020/07/13/231857">マイナンバーカード交付状況のPDFをスクレイピング・CSV変換</a>参照。</p>

<p>もう一つ事例を<a href="https://twitter.com/AkiraOkumura/status/1289787031644561410">教えていただいた</a>。<a href="https://www.pref.aichi.jp/site/covid19-aichi/kansensya-kensa.html">愛知県内の感染者・検査件数</a>からリンクされている「愛知県内発生事例一覧」PDF（例えば今日なら <a href="https://www.pref.aichi.jp/uploaded/attachment/342317.pdf">342317.pdf</a>）である。「長久手市」が「⻑久⼿市」に，「瀬戸市」が「瀬⼾市」に，「西尾市」が「⻄尾市」になってしまっている（ブラウザでは違いが見えにくいが⻑⼿⼾⻄が違う）。こちらはDistillerではなく &lt;&lt; ... /Creator (DocuWorks PDF Driver 7.0.4) /Producer (DocuWorks PDF Build 9)&gt;&gt; となっている。プリンタドライバ経由でPDF化したもののようだ。</p>

<p>いずれにしても，次のようなフィルタで正規化できる：</p>

<pre>
#! /usr/bin/env python3

import fileinput
import unicodedata

tbl = str.maketrans("⺟⺠⻁⻄⻑⻘⻝⻤⻨⻩⻫⻭⻯⻲戶黑", "母民虎西長青食鬼麦黄斉歯竜亀戸黒")

for line in fileinput.input():
    line = unicodedata.normalize('NFKC', line)
    line = line.translate(tbl)
    print(line, end="")
</pre>

<p>NFKC化の副作用？として，（）が()になるなど，全角の英数字・記号類が半角になる。</p>

<p><a href="https://imabari.hateblo.jp/entry/2020/08/03/220407">表引きだけの方法</a>を教えていただいた：</p>

<pre>
tbl = str.maketrans(
    "⺃⺅⺉⺋⺎⺏⺐⺒⺓⺔⺖⺘⺙⺛⺟⺠⺡⺢⺣⺦⺨⺫⺬⺭⺱⺲⺹⺾⻁⻂⻃⻄⻍⻏⻑⻒⻖⻘⻟⻤⻨⻩⻫⻭⻯⻲⼀⼁⼂⼃⼄⼅⼆⼇⼈⼉⼊⼋⼌⼍⼎⼏⼐⼑⼒⼓⼔⼕⼖⼗⼘⼙⼚⼛⼜⼝⼞⼟⼠⼡⼢⼣⼤⼥⼦⼧⼨⼩⼪⼫⼬⼭⼮⼯⼰⼱⼲⼳⼴⼵⼶⼷⼸⼹⼺⼻⼼⼽⼾⼿⽀⽁⽂⽃⽄⽅⽆⽇⽈⽉⽊⽋⽌⽍⽎⽏⽐⽑⽒⽓⽔⽕⽖⽗⽘⽙⽚⽛⽜⽝⽞⽟⽠⽡⽢⽣⽤⽥⽦⽧⽨⽩⽪⽫⽬⽭⽮⽯⽰⽱⽲⽳⽴⽵⽶⽷⽸⽹⽺⽻⽼⽽⽾⽿⾀⾁⾂⾃⾄⾅⾆⾇⾈⾉⾊⾋⾌⾍⾎⾏⾐⾑⾒⾓⾔⾕⾖⾗⾘⾙⾚⾛⾜⾝⾞⾟⾠⾡⾢⾣⾤⾥⾦⾧⾨⾩⾪⾫⾬⾭⾮⾯⾰⾱⾲⾳⾴⾵⾶⾷⾸⾹⾺⾻⾼⾽⾾⾿⿀⿁⿂⿃⿄⿅⿆⿇⿈⿉⿊⿋⿌⿍⿎⿏⿐⿑⿒⿓⿔⿕戶黑",
    "乚亻刂㔾兀尣尢巳幺彑忄扌攵旡母民氵氺灬丬犭罒示礻罓罒耂艹虎衤覀西辶阝長镸阝青飠鬼麦黄斉歯竜亀一丨丶丿乙亅二亠人儿入八冂冖冫几凵刀力勹匕匚匸十卜卩厂厶又口囗土士夂夊夕大女子宀寸小尢尸屮山巛工己巾干幺广廴廾弋弓彐彡彳心戈戸手支攴文斗斤方无日曰月木欠止歹殳毋比毛氏气水火爪父爻爿片牙牛犬玄玉瓜瓦甘生用田疋疒癶白皮皿目矛矢石示禸禾穴立竹米糸缶网羊羽老而耒耳聿肉臣自至臼舌舛舟艮色艸虍虫血行衣襾見角言谷豆豕豸貝赤走足身車辛辰辵邑酉釆里金長門阜隶隹雨靑非面革韋韭音頁風飛食首香馬骨高髟鬥鬯鬲鬼魚鳥鹵鹿麥麻黃黍黒黹黽鼎鼓鼠鼻齊齒龍龜龠戸黒",
)
</pre>

<p>ものかのさんの<a href="https://tama-san.com/resolve-kanji/">やっかいな漢字 – CJK部首補助／康煕部首</a>が参考になる。</p>

<hr>

<p><a href="../" rel="author">奥村 晴彦</a></p>

<p>
<!-- hhmts start -->
Last modified: <time>2024-06-02 16:48:42 JST</time>
<!-- hhmts end -->
</p>
</body>
</html>
