<!DOCTYPE html>
<html lang="ja">
<head>
<link rel="canonical" href="https://okumuralab.org/~okumura/python/logisticregression.html">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>ロジスティック回帰</title>
<link rel="stylesheet" href="style.css">
<link rel="license" href="https://creativecommons.org/licenses/by/4.0/">
<script src="../js/load-mathjax.js" async></script>
</head>
<body>

<nav id="breadcrumbs">
<a href="../">ホーム</a> &gt;
<a href="./">Python</a> &gt;
</nav>

<h1>ロジスティック回帰</h1>

<h2>はじめに</h2>

<p>Python でロジスティック回帰をするには、少なくとも3通りの方法があります：</p>

<ul>
  <li><a href="https://scikit-learn.org">scikit-learn</a> の <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">LogisticRegression()</a></li>
  <li><a href="https://www.statsmodels.org">statsmodels</a> の <a href="https://www.statsmodels.org/stable/glm.html">Generalized Linear Models</a></li>
  <li><a href="https://pingouin-stats.org">Pingouin</a> の <a href="https://pingouin-stats.org/build/html/generated/pingouin.logistic_regression.html">pingouin.logistic_regression</a></li>
</ul>

<p>ここでは scikit-learn を使うことにします。</p>

<p>Rの<a href="../stat/logistic.html">ロジスティック回帰</a>もご覧ください。</p>

<h2>合格の予測（1次元）</h2>

<p>100点満点の模擬テストを20人が受験しました。点数は次の通りです：</p>

<pre>
x = [4, 5, 17, 17, 19, 19, 29, 41, 45, 47, 49, 54, 56, 58, 70, 76, 88, 88, 93, 97]
</pre>

<p>この20人がその後実際の試験を受験して、合否が判明しました。合 = 1、否 = 0 で表すと、次のようになりました：</p>

<pre>
y = [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1]
</pre>

<p>プロットしてみましょう：</p>

<pre>
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 3))  # 図の横・縦。なくてもよい
plt.plot(x, y, "ok")
</pre>

<figure><img src="img/230617a.svg" alt=""></figure>

<p>x から y を予測する式を求めたいのですが、通常の線形回帰ではうまくあてはまりませんし、そもそも何を予測したいのかはっきりしません。</p>

<p>予測するものを、模擬テストの点数が x のときの合格確率だとしましょう。確率は負になったり 1 を超えたりしませんので、線形回帰ではうまくありません。うまく 0 から 1 の範囲におさまってくれる関数はないでしょうか。</p>

<p>例えば</p>

\[ y = \frac{1}{1 + \exp(-x)} = \frac{1}{1 + e^{-x}} \]

<p>はどうでしょうか。これをグラフにすると、次のようになります：</p>

<pre>
import numpy as np

a = np.linspace(-5, 5)
b = 1 / (1 + np.exp(-a))
plt.figure(figsize=(8, 3))
plt.plot(a, b)
</pre>

<figure><img src="img/230617c.svg" alt=""></figure>

<p>これなら縦軸の値は必ず 0 から 1 の範囲に収まりますので、確率と解釈することができます。実際には、このグラフの中心（変曲点の位置）と曲がり具合とを変えられるようにした</p>

\[ y = \frac{1}{1 + \exp(-(w_1 x + w_0))} \]

<p>という式で表される曲線を使うことにします。</p>

<p>この形の曲線をロジスティック曲線といいます。より一般に、0 から 1 に滑らかに変化する S 字形をつぶしたような曲線をシグモイド曲線といいます。シグモイド曲線は、ニューラルネットの活性化関数として使われてきました（現在は ReLU の類を使うのが普通です）。</p>

<p>$x$ が与えられたとき $y = 1$ になる確率 $p$ が</p>

\[ p = \frac{1}{1 + \exp(-(w_1 x + w_0))} \]

<p>で近似できるような係数 $w_1$、$w_0$ を求める方法をロジスティック回帰といいます。ただ、線形の回帰分析のときのような最小2乗法ではなく、最尤法を使います。つまり、$y = 1$ になるデータ点での $p$ と $y = 0$ になるデータ点の $1-p$ を全部掛け算したものが最大になるように係数を決めます。</p>

<p>上の式を書き直すと</p>

\[ \log\left(\frac{p}{1-p}\right) = w_1 x + w_0 \]

<p>となりますが、この左辺に現れる $p$ の関数をリンク関数といいます。この場合のリンク関数はロジット関数と呼ばれ、$\mathrm{logit}(p)$ と書くことがあります。ちなみに $p/(1-p)$ はオッズ（odds）と呼ばれる量ですので、ロジット関数はオッズの対数ということになります。</p>

<p>scikit-learn ではロジスティック回帰は次のようにして行います：</p>

<pre>
from sklearn.linear_model import LogisticRegression

X = np.array(x).reshape(-1, 1)
model = LogisticRegression(penalty=None).fit(X, y)
print(model.coef_, model.intercept_)
</pre>

<p>ここで、<code>np.array()</code> は Python のリストを NumPy の配列に変換する関数です。これに <code>.reshape(m, n)</code> を付けると <code>m</code> 行 <code>n</code> 列の行列になります。<code>m</code> と <code>n</code> はどちらか一つを指定すれば十分です。指定しない側を <code>-1</code> で表します。つまり <code>.reshape(-1, 1)</code> は1列だけの行列に変換します。ここでは行列を表す変数は大文字にしています。<code>np.array(x).reshape(-1, 1)</code> は <code>np.column_stack([x])</code> と書くこともできます。</p>

<p>結果は次のようになります：</p>

<pre>
[[0.08318673]] [-3.56667708]
</pre>

<p>これが $w_1$、$w_0$ の値です。つまり、</p>

\[ p = \frac{1}{1 + \exp(-(0.08319 x - 3.56668))} \]

<p>ということになります。元のデータとこの確率を重ねてグラフにしてみます：</p>

<pre>
plt.figure(figsize=(8, 3))

plt.plot(x, y, "ok")
a = np.arange(0, 101)
plt.plot(a, 1 / (1 + np.exp(-(0.08319 * a - 3.56668))))
</pre>

<figure><img src="img/230617d.svg" alt=""></figure>

<div class="note">
<p>scikit-learn の <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">LogisticRegression()</a> は、デフォルトでは、係数が 0 に近くなるような正則化（regularization）が行われます。ベイズ推定のことばでいえば、係数が 0 に近くなるような事前分布が設定されています。正則化のパラメータ <code>C</code> はデフォルトでは 1 に設定されており、この値が小さいほど強い正則化が行われます。昔の scikit-learn では、正則化をやめるには <code>C</code> に非常に大きな値（例えば <code>C=np.inf</code>）を設定するしかありませんでした。scikit-learn 0.21 で <code>penalty='none'</code> というオプションで正則化を無効にできるようになりました。scikit-learn 1.2 で <code>penalty='none'</code> は廃止予定（deprecated）になり、今後は <code>penalty=None</code> を使うことが推奨されています。</p>
</div>

<h2>合格の判定（2次元）</h2>

<p>次の表を考えましょう。ここで $y$ は入試の合否（合格なら1、不合格なら0）、$x_1$ は内申点、$x_2$ は模試偏差値を表すとします。</p>

<table border="1" cellspacing="0" style="border-collapse:collapse">
  <tbody>
    <tr><th>$y$</th><th>$x_1$</th><th>$x_2$</th></tr>
    <tr><td>0</td><td>3.6</td><td>60.1</td></tr>
    <tr><td>1</td><td>4.1</td><td>52.0</td></tr>
    <tr><td>0</td><td>3.7</td><td>62.5</td></tr>
    <tr><td>0</td><td>4.9</td><td>60.6</td></tr>
    <tr><td>1</td><td>4.4</td><td>54.1</td></tr>
    <tr><td>0</td><td>3.6</td><td>63.6</td></tr>
    <tr><td>1</td><td>4.9</td><td>68.0</td></tr>
    <tr><td>0</td><td>4.8</td><td>38.2</td></tr>
    <tr><td>1</td><td>4.1</td><td>59.5</td></tr>
    <tr><td>0</td><td>4.3</td><td>47.3</td></tr>
  </tbody>
</table>

<p>さきほどと同様に、$x_1$、$x_2$ が与えられたとき $y = 1$ になる確率 $p$ が</p>

\[ p = \frac{1}{1 + \exp(-(w_1 x_1 + w_2 x_2 + w_0))} \]

<p>で近似できるようにパラメータ $w_1$、$w_2$、$w_0$ を定めます。</p>

<p>まずはデータを Python で表します：</p>

<pre>
x1 = [3.6, 4.1, 3.7, 4.9, 4.4, 3.6, 4.9, 4.8, 4.1, 4.3]
x2 = [60.1, 52.0, 62.5, 60.6, 54.1, 63.6, 68.0, 38.2, 59.5, 47.3]
y = [0, 1, 0, 0, 1, 0, 1, 0, 1, 0]
</pre>

<p>このデータについてロジスティック回帰を行うには</p>

<pre>
X = np.column_stack([x1, x2])
model = LogisticRegression(penalty=None).fit(X, y)
print(model.coef_, model.intercept_)
</pre>

<p>と打ち込みます。結果は</p>

<pre>
[[1.27158497 0.06424052]] [-9.44589515]
</pre>

<p>つまり $\mathrm{logit}(p) = 1.27158x_1 + 0.06424x_2 - 9.44590$ で予測されることがわかります。</p>

<hr>

<p><a href="../" rel="author">奥村 晴彦</a></p>

<p>
<!-- hhmts start -->
Last modified: <time>2023-06-17 21:25:59 JST</time>
<!-- hhmts end -->
</p>
</body>
</html>
