<!DOCTYPE html>
<html lang="ja">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<title>文字コード</title>
<link rel="stylesheet" href="style.css">
<link rel="license" href="http://creativecommons.org/licenses/by/4.0/">
</head>
<body>

<nav id="breadcrumbs">
<a href="/~okumura/">ホーム</a> &gt;
<a href="./">Python</a> &gt;
</nav>

<h1>文字コード</h1>

<h3>文字コード変換</h3>

<p>（文字コードと <a href="https://docs.python.org/3/library/codecs.html">codecs</a> モジュールについてまとめる予定）</p>

<h3>ZIPアーカイブのファイル名文字化け対策</h3>

<p>ZIP アーカイブの操作は <a href="https://docs.python.org/3/library/zipfile.html">zipfile</a> パッケージでできる。ただ，日本語環境の Windows で作成したアーカイブは，ファイル名が CP932（Shift JIS の Microsoft 拡張）になっている。これを Mac や Linux で展開するには，UTF-8 に変換しなければならない。</p>

<p>Python 3 の zipfile モジュール（zipfile.py）では</p>

<pre>
<code>            if zinfo.flag_bits &amp; 0x800:
                # UTF-8 filename
                fname_str = fname.decode("utf-8")
            else:
                fname_str = fname.decode("cp437")
</code></pre>

<p>のような処理がされている。コードページ 437（DOS Latin US）はオリジナルの IBM PC の文字セットである。したがって，これを UTF-8 に変換して展開するには，次のようにすればよさそうである（DoraTeX さんによる暗号化 ZIP 対応をマージした）：</p>

<pre>
<code>#! /usr/bin/env python3

import sys
from zipfile import ZipFile
from getpass import getpass

with ZipFile(sys.argv[1]) as z:
    for zinfo in z.infolist():
        if zinfo.flag_bits &amp; 0x1:
            password = getpass('PASSWORD: ')
            z.setpassword(password.encode('utf-8'))
        if not zinfo.flag_bits &amp; 0x800:
            try:
                name = zinfo.filename.encode('cp437').decode('cp932')
            except:
                name = zinfo.filename.encode('cp437').decode('utf-8')
            zinfo.filename = name
        print("Extracting", zinfo.filename)
        z.extract(zinfo)
</code></pre>

<p>この19行ほどのファイルを <code>unzip.py</code> という名前で作成して実行可能にしておけば，<code>./unzip.py アーカイブ名.zip</code> で展開できる。</p>

<p>Mac の展開ツール（ZIP アーカイブをダブルクリックすると現れる）はファイル名の文字コードの自動判断ができる（うまくいかない場合があるという話も聞いた）。Windows 8 以降の展開ツールも大丈夫らしい。Windows 7 は <a href="https://support.microsoft.com/ja-jp/help/2704299/japanese-characters-in-file-names-are-displayed-as-garbled-text-after">KB2704299</a> で対応したらしい。Mac の /usr/bin/unzip にはファイル名変換機能はない（CentOS 7 の /usr/bin/unzip には -O cp932 オプションがある）。</p>

<p>[追記] DoraTeX さんが<a href="https://gist.github.com/doraTeX/0d79d8ff35710a27bc3e5176cc646b60">改良版</a>を作られた。</p>

<h3>NFC，NFD</h3>

<p>Mac の場合，ファイル名の Unicode 正規化がほぼ NFD という形式になっている。例えばカレントディレクトリに「パンダ.txt」というファイルを作り，<a href="https://docs.python.org/3/library/pathlib.html">pathlib</a> でファイル名を読み出す：</p>

<pre>
<code>!touch パンダ.txt  # カレントディレクトリに「パンダ.txt」を作る

import pathlib

path = pathlib.Path(".")
s = str(list(path.glob("*ン*.txt"))[0])
</code></pre>

<p>ところが</p>

<pre>
<code>In [ ]: s
Out[ ]: 'パンダ.txt'

In [ ]: list(s)
Out[ ]: ['ハ', '゚', 'ン', 'タ', '゙', '.', 't', 'x', 't']
</code></pre>

<p>このように濁点・半濁点が分離されている。これが NFD である。通常の NFC に直すには <a href="https://docs.python.org/3/library/unicodedata.html">unicodedata</a> モジュールを使う：</p>

<pre>
<code>import unicodedata

t = unicodedata.normalize('NFC', s)
</code></pre>

<p>とする。今度は</p>

<pre>
<code>In [ ]: t
Out[ ]: 'パンダ.txt'

In [ ]: list(t)
Out[ ]: ['パ', 'ン', 'ダ', '.', 't', 'x', 't']
</code></pre>

<p>見かけは同じだが中身が違う。</p>

<p>別の例（Pokémon の é）：</p>

<pre>
<code>In [ ]: list('é')
Out[ ]: ['é']

In [ ]: unicodedata.normalize('NFD', 'é')
Out[ ]: 'é'

In [ ]: list(unicodedata.normalize('NFD', 'é'))
Out[ ]: ['e', '́']
</code></pre>

<h3>「埼玉」と「埼⽟」</h3>

<p>総務省の<a href="https://www.soumu.go.jp/kojinbango_card/">マイナンバー制度とマイナンバーカード</a>にある「マイナンバーカード交付状況」PDFで「埼玉」（玉: U+7389）と「埼⽟」（⽟: U+2F5F）が混在しているという<a href="https://twitter.com/hal_sk/status/1281853581218336768">話</a>を聞いた。後者はCJK部首/康熙部首である。こういうのは <code>unicodedata.normalize('NFKC', '「埼玉」と「埼⽟」')</code> で統一できる（<code>'NFKD'</code> だと濁点などが分離する）。こういったものが混在する原因はAdobe DistillerまたはChromeのPDF出力である<a href="http://twitter.com/MurakamiShinyu/status/1282590800094756864">とのこと</a>。</p>

<p>NFKC化で全部の部首が正規化されるわけではないようだ。次のようにすればよいと<a href="https://twitter.com/imabari_ehime/status/1282605117468897280">教えていただいた</a>：</p>

<pre>
<code>str.maketrans("⺟⺠⻁⻄⻑⻘⻝⻤⻨⻩⻫⻭⻯⻲戶", "母民虎西長青食鬼麦黄斉歯竜亀戸")
</code></pre>

<p>詳しくは<a href="https://imabari.hateblo.jp/entry/2020/07/13/231857">マイナンバーカード交付状況のPDFをスクレイピング・CSV変換</a>参照。</p>

<p>もう一つ事例を<a href="https://twitter.com/AkiraOkumura/status/1289787031644561410">教えていただいた</a>。<a href="https://www.pref.aichi.jp/site/covid19-aichi/kansensya-kensa.html">愛知県内の感染者・検査件数</a>からリンクされている「愛知県内発生事例一覧」PDF（例えば今日なら <a href="https://www.pref.aichi.jp/uploaded/attachment/342317.pdf">342317.pdf</a>）である。「長久手市」が「⻑久⼿市」に，「瀬戸市」が「瀬⼾市」に，「西尾市」が「⻄尾市」になってしまっている（ブラウザでは違いが見えにくいが⻑⼿⼾⻄が違う）。こちらはDistillerではなく &lt;&lt; ... /Creator (DocuWorks PDF Driver 7.0.4) /Producer (DocuWorks PDF Build 9)&gt;&gt; となっている。プリンタドライバ経由でPDF化したもののようだ。</p>

<p>いずれにしても，次のようなフィルタで正規化できる：</p>

<pre><code>#! /usr/bin/env python3

import fileinput
import unicodedata

tbl = str.maketrans("⺟⺠⻁⻄⻑⻘⻝⻤⻨⻩⻫⻭⻯⻲戶黑", "母民虎西長青食鬼麦黄斉歯竜亀戸黒")

for line in fileinput.input():
    line = unicodedata.normalize('NFKC', line)
    line = line.translate(tbl)
    print(line, end="")
</code></pre>

<p>NFKC化の副作用？として，（）が()になるなど，全角の英数字・記号類が半角になる。</p>

<p><a href="https://imabari.hateblo.jp/entry/2020/08/03/220407">表引きだけの方法</a>を教えていただいた：</p>

<pre><code>tbl = str.maketrans(
    "⺃⺅⺉⺋⺎⺏⺐⺒⺓⺔⺖⺘⺙⺛⺟⺠⺡⺢⺣⺦⺨⺫⺬⺭⺱⺲⺹⺾⻁⻂⻃⻄⻍⻏⻑⻒⻖⻘⻟⻤⻨⻩⻫⻭⻯⻲⼀⼁⼂⼃⼄⼅⼆⼇⼈⼉⼊⼋⼌⼍⼎⼏⼐⼑⼒⼓⼔⼕⼖⼗⼘⼙⼚⼛⼜⼝⼞⼟⼠⼡⼢⼣⼤⼥⼦⼧⼨⼩⼪⼫⼬⼭⼮⼯⼰⼱⼲⼳⼴⼵⼶⼷⼸⼹⼺⼻⼼⼽⼾⼿⽀⽁⽂⽃⽄⽅⽆⽇⽈⽉⽊⽋⽌⽍⽎⽏⽐⽑⽒⽓⽔⽕⽖⽗⽘⽙⽚⽛⽜⽝⽞⽟⽠⽡⽢⽣⽤⽥⽦⽧⽨⽩⽪⽫⽬⽭⽮⽯⽰⽱⽲⽳⽴⽵⽶⽷⽸⽹⽺⽻⽼⽽⽾⽿⾀⾁⾂⾃⾄⾅⾆⾇⾈⾉⾊⾋⾌⾍⾎⾏⾐⾑⾒⾓⾔⾕⾖⾗⾘⾙⾚⾛⾜⾝⾞⾟⾠⾡⾢⾣⾤⾥⾦⾧⾨⾩⾪⾫⾬⾭⾮⾯⾰⾱⾲⾳⾴⾵⾶⾷⾸⾹⾺⾻⾼⾽⾾⾿⿀⿁⿂⿃⿄⿅⿆⿇⿈⿉⿊⿋⿌⿍⿎⿏⿐⿑⿒⿓⿔⿕戶黑",
    "乚亻刂㔾兀尣尢巳幺彑忄扌攵旡母民氵氺灬丬犭罒示礻罓罒耂艹虎衤覀西辶阝長镸阝青飠鬼麦黄斉歯竜亀一丨丶丿乙亅二亠人儿入八冂冖冫几凵刀力勹匕匚匸十卜卩厂厶又口囗土士夂夊夕大女子宀寸小尢尸屮山巛工己巾干幺广廴廾弋弓彐彡彳心戈戸手支攴文斗斤方无日曰月木欠止歹殳毋比毛氏气水火爪父爻爿片牙牛犬玄玉瓜瓦甘生用田疋疒癶白皮皿目矛矢石示禸禾穴立竹米糸缶网羊羽老而耒耳聿肉臣自至臼舌舛舟艮色艸虍虫血行衣襾見角言谷豆豕豸貝赤走足身車辛辰辵邑酉釆里金長門阜隶隹雨靑非面革韋韭音頁風飛食首香馬骨高髟鬥鬯鬲鬼魚鳥鹵鹿麥麻黃黍黒黹黽鼎鼓鼠鼻齊齒龍龜龠戸黒",
)
</code></pre>

<hr>

<p><a href="../" rel="author">奥村 晴彦</a></p>

<p>
<!-- hhmts start -->
Last modified: <time>2020-08-04 08:14:10</time>
<!-- hhmts end -->
</p>
</body>
</html>
